# Tanay Gavankar# tgavanka# HW2 Question 4# Logistic Regressionfrom __future__ import divisionimport collectionsimport copyimport csvimport operatorimport mathimport randomimport timedef print_timing(func):    def wrapper(*arg):        t1 = time.time()        res = func(*arg)        t2 = time.time()        print '%s took %0.3f ms' % (func.func_name, (t2-t1)*1000.0)        return res    return wrapper# sigmoiddef h(W, X):    return 1 / (1 + math.exp(-1*dotp(W, X)))    # dot productdef dotp(v1, v2):    return sum(map(operator.mul, v1, v2))    # log likelinessdef ll(W, td):    return sum(doc['label']*math.log(h(W, doc['data'])) + (1-doc['label'])*math.log(1-h(W, doc['data'])) for doc in td)    class LogisticRegression():    def __init__(self, step=0.001, l=0.01):        self.v = step        self.lm = l        self.text = []        self.sVoc = 0         @print_timing    def train(self, custname="", subset=None):        #print "start reading"        word_indices = [line.strip() for line in open("data/word_indices" + custname + ".txt")]        train_labels = [line.strip() for line in open("data/train_labels.txt")]        training_docs = []        train_pos = 0        with open("data/train" + custname + ".csv") as csvfile:            training = csv.reader(csvfile)            for row in training:                training_docs.append({                'label': int(train_labels[train_pos]),                'data': [int(e) for e in row]})                train_pos += 1        #print "finished reading"        if subset is not None:            training_docs = random.sample(training_docs, subset)                self.sVoc = len(word_indices)        W = [0] * self.sVoc                end_loop = False        loop_count = 0        prevW = copy.deepcopy(W)        iter = 0        while True:            #print "Iteration: %s" % iter            for doc in training_docs:                                self.update(W, doc['data'], doc['label'], self.v)            #print "Log Likelihood: %s" % ll(W, training_docs)            dist = math.sqrt(sum(map(lambda x,y: (x-y)**2, prevW, W)))            #print "Distance: %s" % dist            prevW = copy.deepcopy(W)            if dist < 0.15: # has converged                break            iter += 1        return W        # Update W and return distance moved    def update(self, W, X, l, v):        delta = 0        hf = h(W, X)        for x in xrange(len(X)):            step = self.v * (l - hf) * X[x]            W[x] += step            delta += step**2        return math.sqrt(delta)            def classify(self, W, X):        return 1 if h(W, X) > 0.5 else 0        @print_timing    def test_classify(self, W, file="test"):        test_labels = [line.strip() for line in open("data/" + file + "_labels.txt")]        pred_labels = []        with open("data/" + file + ".csv") as csvfile:            test = csv.reader(csvfile)            for row in test:                pred_labels.append(self.classify(W, [int(x) for x in row]))                res = [(0, 1)[int(a[0]) == a[1]] for a in zip(test_labels, pred_labels)]        print sum(res) / len(res)        if __name__ == "__main__":         for i in xrange(90):        lr = LogisticRegression()        W = lr.train("_feat100", i*50+20)        lr.test_classify(W, "train_feat100")        lr.test_classify(W, "test_feat100")